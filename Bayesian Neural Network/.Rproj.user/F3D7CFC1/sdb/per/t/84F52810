{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Bayesian Neural Network Simulation\"\nauthor: \"Umbertol Junior Mele 1388371\"\ndate: \"20 settembre 2017\"\noutput: pdf_document\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n## Introduction\n\nThese Simulation are made to see an alternative way to fit Neural Networks instead of using the usual methods (like back-propagation or resilient back-propagation).\nSo to reach this goal I used three different ways to set the priors for the weights:\n\n1.  The proposed, by Muller and Rios Insua, three-stage hierarchical priors\n\n2.  Neal (1996) suggested priors.\n\n3.  Conjugate Piors for weights of the output layer and precision, while limited-flat\n    prior for the hidden layer weights.\n    \nAt the end of simulations, it's been commputed also the p-value for the observed data given the posterior predictive simulations; and it's been computed also the AIC and BIC.\n\n\n# First proposed priors\n\nMost standard priors for neural networks in the literature are hierarchical proper priors, because there are useful for neural networks for the lack of interpretability of the parameters. As the addition of levels to the prior reduces the influence of the particular choice made at the top level, the resulting prior at the bottom level (the original parameter) will be more diffuse, more closely macthing the lack of information we really have about the parameters themselves. This approach lets the data have more influence on the posterior.\n\nLet's us take a look on Muller and Rios Insua (1998) proposed priors:\n\n![](muller_rios.png).\n\n\\newpage\n\nPrior distributions, many of them, are multivariate, and are chosen to be conditionally conjugate, which will help in fitting the model.\n\nThe distribution for the parameters and hyperparameters are\n\n![](muller_rios_model.png).\n\nwhere $\\alpha_{\\beta}$, $A_{\\beta}$,$\\alpha_{\\gamma}$, $A_{\\gamma}$,\n$c_{\\beta}$, $C_{\\beta}$,,$c_{\\gamma}$, $C_{\\gamma}$, $s$ and $S$ are costants that need to be chosen a priori.\n\n\\newpage\n\n# Second proposed priors\n\nNeal suggests a more complex model, although all parameters are univariate. He also use hyperbolic tangent activation functions rather than logistic functions.\n\nA simple version of the model is:\n\n![](neal_model.png).\n\nwhere $\\alpha_0$, $\\alpha_1$, $\\alpha_2$, $\\alpha_a$, $\\alpha_b$, $\\omega_0$, $\\omega_a$ and $\\omega_b$ are constants to be specified. A DAG diagram off the model is shown in next page.\n\n\\newpage\n\n![](neal_dag.png).\n\nEach of the original network parameters ($\\beta$ and $\\gamma$) is treated as a univariate normal with mean zero and its own variance. These variances are the product of two hyperparameters, one for the originating node of the link in the graph and one for the destination node.\n\nFor example, the weight for the first input to the first hidden node, $\\gamma_{11}$, has distribution $N(0, \\sigma_{in, 1}* \\sigma_{a,1})$, where $\\sigma_{in,h}$ is the term for the link from the __h__-th input, and $\\sigma_{a,j}$ is the term for the links into the __j__-th hidden node; the weight from the first hidden node to the output (i.e. the regression coefficient), $\\beta_1$, has distribution $N(0, \\sigma_{out,1}*\\sigma_{0})$,\nwhere $\\sigma_{out,j}$ is the term for the links from the __j__-th hidden node, and $\\sigma_0$ is the term for links to the output node. For all off the new $\\sigma$ parameters and for the original $\\sigma$ of the error term, there's an inverse-gamma distribution.\n\nThe version of the model presented here is for a univariate response; this model extends to multivariate responses and classification by adding an additional output unit for each dimension of the response and extending the hierarchy to account for additional output units. One further note on this model is that Neal also discusses using t distribution instead of normal distributions for the parameters, resulting in a more robust model.\n\n\\newpage\n\n# Third proposed model.... Conjugate priors for beta and variance and flat prior for gamma\n\nAs we know the last layer of a neural network is a linear regression on the hidden layer outputs, so let's assume that the X in input on the NN becomes Z in the hidden layer (so for fixed $\\gamma$ we have a linear on last layer).\n\n$$z_{ij} = \\Bigg[ 1 + exp\\big( - \\gamma_{j0} + \\sum_{h=1}^r \\gamma_{jh}*x_{ih}\\big)\\Bigg]$$\n\nSo the ouput layer is fitting a linear regression in this Z data.\n\n$$y_i = z_i^T \\beta + \\epsilon_i$$\nwith $\\epsilon \\sim N(0,\\sigma^2)$, so the likelihood function is :\n\n$$p(y|Z,\\beta, \\sigma^2) \\propto (\\sigma^2)^{-n/2} \\cdot exp \\Big( - \\frac{1}{2\\sigma^2}(y - Z\\beta)^T(y - Z\\beta)\\Big)$$\n\nand the orinary least squares solution is: $\\hat\\beta = (Z^T Z )^{-1} Z^Ty$ \n\nUsing a little trick:\n\n$$(y - Z\\beta)^T(y - Z\\beta) = (y - Z\\hat\\beta)^T(y - Z\\hat\\beta) + (\\beta- \\hat\\beta)^T(Z^TZ)(\\beta- \\hat\\beta)$$\nSo the likelihood is now re-written as:\n\n$$p(y|Z,\\beta, \\sigma^2) \\propto (\\sigma^2)^{-\\frac{n - k}{2}} exp\\Big( - \\frac{(y - Z\\hat\\beta)^T(y - Z\\hat\\beta)}{2\\sigma^2} \\Big) \\cdot (\\sigma^2)^{-\\frac{k}{2}} exp\\Big( - \\frac{(\\beta- \\hat\\beta)^T(Z^TZ)(\\beta- \\hat\\beta)}{2\\sigma^2} \\Big)$$\n\nThis suggests a form for the prior:\n$$p(\\sigma^2,\\beta) = p(\\sigma^2)p(\\beta|\\sigma)$$\n\nWhere $p(\\sigma^2)$ is an inverse-gamma distribution with $a_0 = \\frac{n_0-k_0}{2}$ and $b_0 = \\frac{SSE_0}{2}$,\n\nwhile $p(\\beta|\\sigma^2)$ is a normal distribution $N(\\mu_0, \\sigma^2\\Lambda_0^{-1})$\n\nSo follows that the posterior distribution are $N(\\mu_n, \\sigma^2 \\Lambda_n^{-1})$ and $In$-$Gamma(a_n, b_n)$ distributions, with parameters given by:\n\n$$\\mathbf{\\Lambda_n = (Z^TZ +\\Lambda_0), \\qquad \\mu_n =(\\Lambda_n)^{-1}(Z^Ty + \\Lambda_0\\mu_0)}$$\n$$a_n= a_0+\\frac{n}{2} ,\\qquad b_n = b_0+ \\frac{1}{2}(\\mathbf{y}^T\\mathbf{y} + \\mathbf{\\mu}_0^T\\mathbf{\\Lambda}_0\\mathbf{\\mu}_0 - \\mathbf{\\mu}_n^T \\mathbf{\\Lambda}_n\\mathbf{\\mu}_n)$$\n\n\nSo everythings looks easy for Beta and the variance, but the priors for the $\\gamma$ parameters is a more tender question, as they are the ones that are particulary lacking in interpretation. One obvious approach is to use a flat prior for them, but since this prior is improper and leads to an improper posterior (it turns out that there are two ways in which things can go wrong: linear independence and tail behavior), we restrict this priors. The discussion of this prior first appeared in Lee (2003).\n\n\\newpage\n\nTo understand the linear independence problem, let's see how the posterior will be proper as long as the design matrix $Z$ is full rank. Thus, for a neural network, we need the k logistic basis function to be linearly independent. A straightforward way to ensure linear independence is to require that the determinant of $Z^TZ$ is positive, or for computational purpose the determinant should be larger than some small positive number C.\n\nThe second possible posterior propriety is that the likelihood does not necessarily go to zero in the tails, converging to various nonzero constant. If the tails of the prior also do not go to zero as the parameter  values go off to infinity, the tails of the posterior will similary fail to go to zero, and thus the posterior will not have finite integral unless the parameter space is bounded, so wee want that $|\\gamma_{jh}|<D$.\n\n\n### Proof that the posterior is a proper distribution\n\n\n\n\nSo we can assume that also for gamma we have a finite posterior distribution that this time doesn't appear in the parametric family distributions... so we need to implement for this last variables a Metropolis-Hasting simulation using simmetric proposal distribution to make things easier.\n\n\n\\newpage\n\n## Implementation\n\n1) First case was simulated with WinBugs, is a simulation of 10000 iteration with thin equal to 7.\n\n2) Second case was simulated with Jags, there are more simulation of 10000 and 100000 iteration with thin equal to 1.\n\n3) Third case was simulated in R and is a simulation of 10000 iteration with thin equal to 5.\n\n\n\n\n## Convergence Diagnostics\n\n*\"a weak diagnostic is better than no diagnostic at all\" (Cowles & Carlin, 1996)*\n\nTo make Bayesian analysis we need to be sure that the chain is converged to the invariant distribution (posterior distribution), so to reach this goal we need to do some convergence diagnostic; and I made:\n\n1)  Time-series Plots for the parameters of interest: that needs to be wiggly to make sure that the chain has traversed different parts of the sample space or if there is a clean pattern in such a plot, the MCMC algorithm may not have converged.\n\n2)  Running-mean plots for the parameters of interest: if the algorithm has converged, the running mean should stabilize at the posterior mean for each parameter. However, these plots look only at the mean of the parameters and hence are inadequate.\n\n3)  Plot of auto-correlation function: An MCMC algorithm generating higlhy autocorrelated parameter values will need a large number of iterations to be able to traverse the whole sample space of the parameters.\n\n4) Cross-correlation plots: high values of this last mesure may indicate the need for a reparametrization of the model.\n\n5)Spectral Analysis:\n  - **'Geweke Convergence diagnostic'** that make a Z-statitic using the mean of first 10% of the chain whit the 50% of the last part of the chain.\n  -**'Heidelberger and Welch Convergence diagnostic'** that calculate the test statistics on the whole chain to accept or reject the null hypothesis that the chain is from a stationary distribution in the first part, and in the second part the halfwidth test, calculates half the width of the (1 - alpha)% probability interval (credible interval) around the mean. If the ratio of the halfwidth and the mean is lower than eps, then the chain passes the halfwidth test. Otherwise, the chain fails the halfwidth test and must be updated for more iterations until sufficient accuracy is obtained.\n\n\n\n\n\n\n\n\\newpage\n\n## References\n\n-   Herbert K. H. Lee \"Bayesian Nonparametrics via Neural Networks\"\n-   Christian P.Robert  George Casella  \"Monte Carlo Statistical Methods\" second edition\n-   Ioannis Ntzoufras  \"Bayesian Modeling Using WinBugs\"\n-   Wikipedia Bayesian linear regression <https://en.wikipedia.org/wiki/Bayesian_linear_regression>\n",
    "created" : 1505979358926.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1311506100",
    "id" : "84F52810",
    "lastKnownWriteTime" : 1506613630,
    "last_content_update" : 1506613630,
    "path" : "C:/Users/Umbertojunior/Desktop/data science/Second Semestr/SDS 2/project/Bayesian Neural Network/presentation/presentation_final_project.Rmd",
    "project_path" : "presentation/presentation_final_project.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}